# Disaster Data Kafka Consumer (LangGraph)

A LangGraph-based autonomous agent that consumes disaster data packets from Kafka and stores them in PostgreSQL for analysis and querying.

## Overview

This agent implements a complete data ingestion pipeline using LangGraph for workflow orchestration. It consumes discrete disaster event packets from Kafka (generated by the disaster data collection agent) and stores them in a structured PostgreSQL database.

## Architecture

### LangGraph Workflow

```
┌─────────────────┐
│ consume_kafka   │  Read messages from Kafka topic
└────────┬────────┘
         │
         ↓
┌─────────────────┐
│ validate_packets│  Validate packet structure and schema
└────────┬────────┘
         │
         ↓
┌─────────────────┐
│ transform_data  │  Transform to PostgreSQL schema
└────────┬────────┘
         │
         ↓
┌─────────────────┐
│store_in_postgres│  Insert into PostgreSQL database
└────────┬────────┘
         │
         ↓
┌─────────────────┐
│update_statistics│  Track consumption metrics
└────────┬────────┘
         │
         ↓
       [END]
```

### Components

1. **Kafka Consumer** - Consumes from `disaster-data-ingestion` topic
2. **Validator** - Validates packet schema and required fields
3. **Transformer** - Converts Kafka packets to database records
4. **PostgreSQL Storage** - Stores disaster events with full indexing
5. **Statistics Tracker** - Monitors consumption metrics

## Features

### Data Ingestion

- Consumes discrete disaster event packets from Kafka
- Batch processing for efficient throughput
- Automatic schema validation
- Deduplication based on packet_id
- Error handling and retry logic

### Database Storage

**disaster_events table**:
- Temporal fields (start_date, end_date, duration)
- Spatial fields (primary_location, affected_locations)
- Impact metrics (deaths, injured, displaced, affected)
- Metadata (severity, source info, relevance score)
- Full-text search capabilities
- Optimized indexes for queries

**consumption_statistics table**:
- Batch metrics
- Processing time tracking
- Disaster type breakdown
- Severity distribution

### Query Capabilities

- Query by disaster type
- Query by location (supports array search)
- Query by date range
- Query by severity
- Aggregate statistics
- Custom SQL queries

### Monitoring

- Real-time consumption statistics
- Invalid packet tracking
- Error logging
- Processing metrics

## Installation

### Dependencies

```bash
# Install Python dependencies
pip install langgraph psycopg2-binary kafka-python

# Or use requirements.txt
pip install -r requirements.txt
```

### PostgreSQL Setup

1. **Install PostgreSQL** (if not already installed):
```bash
# Ubuntu/Debian
sudo apt-get install postgresql postgresql-contrib

# macOS
brew install postgresql
```

2. **Create Database**:
```bash
sudo -u postgres psql
CREATE DATABASE disaster_data;
CREATE USER disaster_user WITH PASSWORD 'your_password';
GRANT ALL PRIVILEGES ON DATABASE disaster_data TO disaster_user;
\q
```

3. **Set Environment Variables**:
```bash
export POSTGRES_HOST=localhost
export POSTGRES_PORT=5432
export POSTGRES_DB=disaster_data
export POSTGRES_USER=disaster_user
export POSTGRES_PASSWORD=your_password
```

### Kafka Setup

1. **Install Kafka** (if not already running):
```bash
# Using Docker
docker run -d --name kafka -p 9092:9092 apache/kafka

# Or download from Apache Kafka website
```

2. **Create Topic**:
```bash
kafka-topics --create \
  --topic disaster-data-ingestion \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1
```

3. **Set Environment Variables**:
```bash
export KAFKA_BOOTSTRAP_SERVERS=localhost:9092
export KAFKA_TOPIC=disaster-data-ingestion
export KAFKA_CONSUMER_GROUP=disaster-consumer-group
```

## Usage

### Basic Usage

```python
from agent import run_disaster_consumer, initialize_database

# Initialize database (first time only)
initialize_database()

# Run consumer for one batch
result = run_disaster_consumer(batch_size=10)

print(f"Consumed: {len(result['kafka_messages'])}")
print(f"Stored: {result['stored_count']}")
print(f"Statistics: {result['statistics']}")
```

### Continuous Consumption (Daemon Mode)

```python
import time
from agent import run_disaster_consumer

print("Starting continuous consumer...")
try:
    while True:
        result = run_disaster_consumer(batch_size=10)
        print(f"Batch: {result['stored_count']} stored, {result['failed_count']} failed")
        time.sleep(5)  # Poll every 5 seconds
except KeyboardInterrupt:
    print("Consumer stopped")
```

### Querying Data

```python
from agent import (
    query_disasters_by_type,
    query_disasters_by_location,
    query_disasters_by_date_range,
    get_statistics_summary
)

# Query by disaster type
floods = query_disasters_by_type("floods")
print(f"Found {len(floods)} flood events")

# Query by location
kerala_events = query_disasters_by_location("Kerala")
print(f"Found {len(kerala_events)} events in Kerala")

# Query by date range
august_events = query_disasters_by_date_range("2024-08-01", "2024-08-31")
print(f"Found {len(august_events)} events in August")

# Get statistics
stats = get_statistics_summary()
print(f"Total events: {stats['total_events']}")
print(f"Total deaths: {stats['total_deaths']}")
print(f"By type: {stats['by_disaster_type']}")
```

### Running Examples

```bash
cd langraph/1_disaster_kafka_consumer
python example_usage.py
```

## Configuration

Edit `config.py` to customize:

### Kafka Configuration

```python
KAFKA_CONFIG = {
    "bootstrap_servers": "localhost:9092",
    "topic": "disaster-data-ingestion",
    "consumer_group": "disaster-consumer-group",
    "batch_size": 10,
}
```

### PostgreSQL Configuration

```python
POSTGRES_CONFIG = {
    "host": "localhost",
    "port": 5432,
    "database": "disaster_data",
    "user": "postgres",
    "password": "postgres",
}
```

### Consumer Behavior

```python
CONSUMER_CONFIG = {
    "batch_size": 10,
    "poll_interval_seconds": 5,
    "max_retries": 3,
    "continuous_mode": True,
}
```

## Database Schema

### disaster_events Table

| Column | Type | Description |
|--------|------|-------------|
| id | SERIAL | Primary key |
| packet_id | VARCHAR(255) | Unique packet identifier |
| disaster_type | VARCHAR(50) | Type of disaster |
| event_start_date | DATE | Event start date |
| event_end_date | DATE | Event end date |
| duration_days | INTEGER | Duration in days |
| primary_location | VARCHAR(255) | Main affected location |
| affected_locations | TEXT[] | Array of affected locations |
| deaths | INTEGER | Number of deaths |
| injured | INTEGER | Number of injured |
| displaced | INTEGER | Number displaced |
| affected | INTEGER | Number affected |
| severity | VARCHAR(20) | Severity level |
| source_url | TEXT | Source URL |
| raw_packet | JSONB | Full packet as JSON |
| created_at | TIMESTAMP | Record creation time |

**Indexes**:
- `idx_disaster_type` - For filtering by disaster type
- `idx_event_dates` - For date range queries
- `idx_locations` - GIN index for location array searches
- `idx_severity` - For severity filtering
- `idx_ingestion_timestamp` - For time-based queries

### consumption_statistics Table

| Column | Type | Description |
|--------|------|-------------|
| id | SERIAL | Primary key |
| batch_timestamp | TIMESTAMP | Batch processing time |
| messages_consumed | INTEGER | Messages consumed |
| messages_stored | INTEGER | Messages stored |
| messages_failed | INTEGER | Messages failed |
| disaster_type_breakdown | JSONB | Stats by disaster type |
| severity_breakdown | JSONB | Stats by severity |
| processing_time_seconds | DECIMAL | Processing time |

## Packet Format

The consumer expects packets in this format (produced by the disaster data collection agent):

```json
{
  "packet_id": "disaster_event_floods_kerala_20240815_001",
  "packet_type": "discrete_disaster_event",
  "kafka_topic": "disaster-data-ingestion",
  "timestamp": "2024-08-15T10:30:00",
  "temporal": {
    "start_date": "2024-08-15",
    "end_date": "2024-08-17",
    "duration_days": 2
  },
  "spatial": {
    "primary_location": "Kerala",
    "affected_locations": ["Kerala", "Wayanad", "Idukki"],
    "location_count": 3
  },
  "impact": {
    "deaths": 25,
    "injured": 50,
    "displaced": 1000,
    "affected": 5000
  },
  "metadata": {
    "disaster_type": "floods",
    "severity": "high",
    "event_name": "Kerala Monsoon Floods 2024",
    "source": {
      "url": "https://example.com",
      "domain": "example.com",
      "title": "News Title"
    },
    "relevance_score": 9
  },
  "processing_instructions": {
    "priority": "high",
    "retention_days": 365
  }
}
```

## Example Queries

### SQL Queries

```sql
-- Get all high-severity floods
SELECT * FROM disaster_events
WHERE disaster_type = 'floods' AND severity = 'high'
ORDER BY event_start_date DESC;

-- Get events in a specific location
SELECT * FROM disaster_events
WHERE 'Kerala' = ANY(affected_locations);

-- Get casualty statistics by disaster type
SELECT disaster_type,
       COUNT(*) as event_count,
       SUM(deaths) as total_deaths,
       SUM(injured) as total_injured
FROM disaster_events
GROUP BY disaster_type;

-- Get events in date range with casualties
SELECT disaster_type, primary_location, event_start_date, deaths
FROM disaster_events
WHERE event_start_date BETWEEN '2024-08-01' AND '2024-08-31'
  AND deaths > 0
ORDER BY deaths DESC;

-- Get monthly disaster trends
SELECT DATE_TRUNC('month', event_start_date) as month,
       disaster_type,
       COUNT(*) as event_count
FROM disaster_events
GROUP BY month, disaster_type
ORDER BY month DESC, event_count DESC;
```

### Python Queries

```python
from agent import get_db_connection

conn = get_db_connection()
cursor = conn.cursor()

# Get recent high-impact events
cursor.execute("""
    SELECT disaster_type, primary_location, event_start_date,
           deaths, injured, severity
    FROM disaster_events
    WHERE severity IN ('high', 'critical')
      AND (deaths > 10 OR injured > 50)
    ORDER BY event_start_date DESC
    LIMIT 20
""")

for row in cursor.fetchall():
    print(f"{row[0]} in {row[1]}: {row[3]} deaths, {row[4]} injured")

cursor.close()
conn.close()
```

## Monitoring and Observability

### Consumption Metrics

```python
from agent import get_statistics_summary

stats = get_statistics_summary()
print(f"Total events ingested: {stats['total_events']}")
print(f"Events by type: {stats['by_disaster_type']}")
print(f"Total impact: {stats['total_deaths']} deaths")
```

### View Recent Statistics

```sql
SELECT * FROM consumption_statistics
ORDER BY batch_timestamp DESC
LIMIT 10;
```

### Check for Errors

```python
result = run_disaster_consumer(batch_size=10)

if result['invalid_packets']:
    print(f"Invalid packets: {len(result['invalid_packets'])}")
    for invalid in result['invalid_packets']:
        print(f"  Reason: {invalid['reason']}")

if result['errors']:
    print(f"Errors: {result['errors']}")
```

## Integration with Disaster Data Collection Agent

This consumer is designed to work with the disaster data collection agent:

```
┌──────────────────────────────────────────────────────────────┐
│                    Complete Pipeline                         │
└──────────────────────────────────────────────────────────────┘

google_adk/5_disaster_data_agent
    │
    ├─ Search web for disaster data
    ├─ Crawl URLs with Crawl4AI
    ├─ Extract structured data
    ├─ Generate discrete event packets
    │
    ↓
Kafka Topic: disaster-data-ingestion
    │
    ↓
langraph/1_disaster_kafka_consumer (THIS AGENT)
    │
    ├─ Consume Kafka packets
    ├─ Validate and transform
    ├─ Store in PostgreSQL
    │
    ↓
PostgreSQL Database: disaster_data
    │
    └─ Query and analysis
```

## Testing

### Mock Mode (No Kafka Required)

```python
from agent import consume_kafka_messages

# Get mock disaster data for testing
messages = consume_kafka_messages(use_mock=True)
print(f"Mock messages: {len(messages)}")
```

### End-to-End Test

```bash
# 1. Initialize database
python -c "from agent import initialize_database; initialize_database()"

# 2. Run consumer
python -c "from agent import run_disaster_consumer; run_disaster_consumer(batch_size=10)"

# 3. Query data
python -c "from agent import get_statistics_summary; print(get_statistics_summary())"
```

## Troubleshooting

### Issue: Cannot connect to PostgreSQL

**Solution**: Check connection settings:
```bash
# Test connection
psql -h localhost -U postgres -d disaster_data

# Check if PostgreSQL is running
sudo systemctl status postgresql
```

### Issue: Cannot connect to Kafka

**Solution**: Verify Kafka is running:
```bash
# Check Kafka
docker ps | grep kafka

# Test topic exists
kafka-topics --list --bootstrap-server localhost:9092
```

### Issue: Database schema not created

**Solution**: Manually initialize:
```python
from agent import initialize_database
initialize_database()
```

### Issue: Duplicate key errors

**Solution**: Packets with same `packet_id` are skipped (ON CONFLICT). Check for:
- Duplicate messages in Kafka
- Consumer group offset issues
- Re-processing old messages

## Performance Optimization

### Batch Size

Adjust batch size based on throughput needs:
```python
# Small batches for low latency
run_disaster_consumer(batch_size=10)

# Large batches for high throughput
run_disaster_consumer(batch_size=100)
```

### Database Indexing

Already optimized with indexes on:
- disaster_type
- event dates
- locations (GIN index for array search)
- severity
- ingestion_timestamp

### Connection Pooling

For production, use connection pooling:
```python
from psycopg2 import pool

connection_pool = pool.SimpleConnectionPool(
    minconn=1,
    maxconn=10,
    **POSTGRES_CONFIG
)
```

## Production Deployment

### Environment Variables

Create `.env` file:
```bash
# PostgreSQL
POSTGRES_HOST=your-db-host
POSTGRES_PORT=5432
POSTGRES_DB=disaster_data
POSTGRES_USER=your-user
POSTGRES_PASSWORD=your-password

# Kafka
KAFKA_BOOTSTRAP_SERVERS=your-kafka-host:9092
KAFKA_TOPIC=disaster-data-ingestion
KAFKA_CONSUMER_GROUP=disaster-consumer-prod

# Environment
ENVIRONMENT=production
LOG_LEVEL=INFO
```

### Systemd Service (Linux)

Create `/etc/systemd/system/disaster-consumer.service`:
```ini
[Unit]
Description=Disaster Data Kafka Consumer
After=network.target

[Service]
Type=simple
User=disaster
WorkingDirectory=/opt/disaster-consumer
ExecStart=/usr/bin/python3 /opt/disaster-consumer/agent.py
Restart=always
EnvironmentFile=/opt/disaster-consumer/.env

[Install]
WantedBy=multi-user.target
```

Start service:
```bash
sudo systemctl enable disaster-consumer
sudo systemctl start disaster-consumer
sudo systemctl status disaster-consumer
```

### Docker Deployment

```dockerfile
FROM python:3.10-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["python", "agent.py"]
```

### Monitoring with Prometheus

Add metrics endpoint:
```python
from prometheus_client import Counter, Histogram, start_http_server

messages_consumed = Counter('disaster_messages_consumed', 'Total messages consumed')
processing_time = Histogram('disaster_processing_seconds', 'Processing time')

# In your consumer loop
messages_consumed.inc(len(messages))
```

## Future Enhancements

- [ ] Add support for partitioned tables (by date)
- [ ] Implement data retention policies
- [ ] Add real-time alerting for critical events
- [ ] Create dashboard for visualization
- [ ] Add data export capabilities (CSV, JSON)
- [ ] Implement backup and recovery procedures
- [ ] Add support for multiple Kafka topics
- [ ] Create REST API for queries
- [ ] Add machine learning for anomaly detection

## Contributing

To extend or modify the consumer:

1. Add new validation rules in `node_validate_packets()`
2. Extend database schema in `initialize_database()`
3. Add custom query functions
4. Enhance error handling
5. Add new monitoring metrics

## License

Part of the agents project.

## Support

For issues or questions:
- Check logs in `logs/disaster_consumer.log`
- Review example usage in `example_usage.py`
- Test with mock mode first
- Verify Kafka and PostgreSQL connectivity

## Acknowledgments

- **LangGraph**: Workflow orchestration framework
- **Apache Kafka**: Message streaming platform
- **PostgreSQL**: Relational database
- **psycopg2**: PostgreSQL adapter for Python
